
\documentclass[format=acmsmall, review=false, screen=true]{acmart}

\usepackage{booktabs} % For formal tables

\usepackage[ruled]{algorithm2e} % For algorithms
\renewcommand{\algorithmcfname}{ALGORITHM}
\SetAlFnt{\small}
\SetAlCapFnt{\small}
\SetAlCapNameFnt{\small}
\SetAlCapHSkip{0pt}
\IncMargin{-\parindent}

% Copyright
\setcopyright{acmlicensed}

% Document starts
\begin{document}
% Title portion. Note the short title for running heads 
\title[Effects of SSDs on Databases]{Effects of Solid State Drives on Database Performance}  
\author{Philip M. Westrich}
\affiliation{
  \institution{Tennessee Technological University}
  \streetaddress{1 William L. Jones Drive}
  \city{Cookeville} \state{TN} \postcode{38501} \country{USA}
}

\begin{abstract}

    In the last decade or so, a new form of flash-based persistent storage, known as solid state drives (SSDs), have begun 
    to replace traditional mechanical hard disk drives (HDDs). They are orders of magnitude faster, lighter, more energy 
    efficient, and less prone to sudden impact than HDDs. As their prices fall and storage capacities increase, they have 
    taken more of a hold in both the consumer and enterprise market. These new SSDs behave differently than HDDs, which can 
    have a significant effect on any read or write heavy workload, such as database management systems. In this paper, we explore 
    what these effects are and their proposed solutions.
 
\end{abstract}

\maketitle

\section{Introduction}

For years, the gap in performance between processors and persistent storage has continually widened. Disk latency for 
traditional hard disk drives (HDDs) has improved by approximately ten percent per year, while we are just now beginning 
to see the end of Moore's Law for processors. \cite{Xie2011}

Traditional HDDs rely on mechanical parts, and therefore are destined to fail eventually, take relative eons to read 
or write data, and are prone to multiuple methods of failure, such as sudden force, vibration, or simply mechanical 
failure. They also are power inefficent and generate pleny of heat. \cite{Xie2011}

With the introduction of solid state drives (SSDs), we can begin to address many of the aforementioned issues, as they 
are not susceptible to the same problems. SSDs are lighter, faster, more energy efficient, and resistant to blunt force.
\cite{Xie2011} 

However, they are not completely free from issue. At the time of writing, SSDs are still much more expensive per gigabyte 
than their HDD counterparts, though they promise to reach equivalence within a few years. They also have much different 
performance characteristics than HDDs, most notably, usually being worse at small random writes. They also tend to become 
slower as they fill up, and have a limited number of read/write/erase cycles they can endure before failing. 
\cite{Xie2011, Dumitru2007}

Overall, SSDs are better than their predecessor, and will most likely come to replace them in the coming years. Therefore, 
applications that are heavily reliant on their storage medium, such as databases, must come prepared for it. In this paper, 
we will explore what effects these performance differences have as well as the solutions that have been proposed.

\section{Overview of solid state drives}

Though solid state drives are designed to emulate the behavior of previous block devices, under the hood, they operate 
quite differently. \cite{Lee2008, Cornwell2012, Micheloni2013, MatejFucek2014} In this section, we explain the general 
architecture of a solid state drive and explain these differences.

Solid state drives consist of three major components: an array of non-volatile flash memory, a host interface, and a 
microcontroller that bridges the gap between the two. 

\subsection{NAND flash}

Most SSDs today use a type of electrically erasable programmable read-only memory (EEPROM) known as NAND flash memory. 
NAND is an abbreviation for 'not-and', a type of logic gate. These NAND cells are arranged into a grid pattern. These 
cells can only be accessed in certain patterns dependent on the particular drive's arrangement of those cells. 
\cite{Cornwell2012, Micheloni2013}

Several of these grids, usually one to four, are placed together into what is known as a die. Each die can read somewhere 
around 400 MB/sec, while they can only write at around 20MB/sec. This is due to the much more complicated write method 
they employ. However, they have a much lower latency for these operations, typically measured in microseconds, and in 
order to meet capacity and speed requirements, multiple dies are packaged inside one SSD. The controller will often 
read and write to multiple dies at once in order to reach performance goals. \cite{Cornwell2012, Micheloni2013}

SSDs use a write-verify method to program, or write to, the cells. In this procedure, the cells are first erased, then 
a high voltage signal is applied to them until they reach the appropriate value. As the cells hold more bits, this 
process gets more difficult and slows down, as the voltages applied to the cells must become more fine-grained and precise.
\cite{Cornwell2012, Micheloni2013}

\subsection{SSD controller}

The controller has many jobs. First, it must present to the host device the same interface HDDs do. Second, it must 
translate the commands issued to it to operations on the NAND flash. Third, it must wear the NAND in the drive evenly 
to prevent premature failure. Fourth, it must deal with the higher error rate that the NAND flash has over HDDs. 
\cite{Cornwell2012, Micheloni2013, MatejFucek2014}

To carry out these tasks, SSDs have a small computer, known as a microcontroller, embedded within them. The microcontroller 
is also paired with specialized hardware to speed up some of its computations, as well as some amount of DRAM to cache and 
buffer events. \cite{Cornwell2012, Micheloni2013, MatejFucek2014}

The controller must present to the host device the same interface other block devices do. However, the NAND flash is not 
accessed in the same way; instead, it has its own filesystem it maintains, known as the flash translation layer, or FTL. 
The FTL is used to perform the translation from the logical blocks the host requests to the pages and blocks within the 
NAND flash. \cite{Cornwell2012, Micheloni2013, MatejFucek2014}

Because cells must be erased before they can be written to, new data is always written to known empty pages, and the old 
pages are marked as garbage in the FTL. Therefore, the actual locations of data may move across the drive as it is written 
to, unlike HDDs. The FTL must maintain a list of empty, written, and garbage pages, and keep track of the mapping from 
the logical blocks requested by the host and their actual location on the SSD. The FTL also maintains the number of writes 
a block has experienced as well as a list of bad blocks that have either started out defective from the factory, or have 
worn out over time. \cite{Cornwell2012, Micheloni2013, MatejFucek2014}

When the controller is idle for some period of time, it will start performing garbage collection on the now invalid pages 
in order to free up more space. Some SSDs, especially earlier ones, would degrade in performance as the drive filled up 
and it ran out of free pages. The drive then had to immediately garbage collect to free up more, causing a delay in writes. 
Modern SSDs will understate their capacity to the host system, that way there are always free pages on the disk. 
\cite{Cornwell2012, Micheloni2013, MatejFucek2014}

NAND flash has a much higher error rate than the magnetic platters of HDDs. Therefore, all SSDs have some amount of 
hardware-assisted error correction. Higher quality drives have more dedicated hardware, while cheaper ones may share 
hardware for a larger number of pages. \cite{Cornwell2012, Micheloni2013, MatejFucek2014}

\section{Effects on database systems}

For years, systems have assumed that HDDs are the form of persistent storage used, and are optimized for it. These 
optimizations often are detrimental to SSD performance. In this section, we explain these assumptions and why they 
may lead to suboptimal performance.

\subsection{Physical differences}

HDDs have multiple moving parts: the platters that spin at some speed, and the heads that move across them that read the 
data from the platters. It takes time to move these parts through space. Often, to conserve power and lifespan, the drive 
stops rotating its platters. When at a complete stop, it can take between four to ten seconds to bring the platters back 
up to speed. This may not be an issue in consumer-grade applications, but it can be a matter of life and death in critical
real-time systems. \cite{MatejFucek2014} The head of the drive must also move to the correct location to read from the 
requested sector. \cite{Cornwell2012}

Solid state drives have zero moving parts, hence their name. They do not require any spin up time, and their seek time is 
both uniform and much, much smaller. An additional benefit is that they consume much less energy. 
\cite{Cornwell2012, Micheloni2013, MatejFucek2014}

\subsection{Differences in access patterns}

Current systems tend to assume that they will be placing their data on to HDDs, and tend to optimize for that; they try 
to make up for the shortcomings of the physical media. They assume the location of the sectors requested are always in 
the same location, and that lower addresses are towards the faster, outside of the drive, and higher addresses are towards 
the inside. \cite{Cornwell2012}

Because of the seek time added by moving the head across the drive and the assumption of block locations, filesystem drivers 
will perform complicated math to determine how it can move the head the minimum distance for a sequence of reads and writes. 
They will also perform \textit{I/O coalescing,} or merging many smaller requests into one larger one, since HDDs are much 
better at large sequential operations. \cite{Cornwell2012}

In order to minimize the number of requests made to the drive, filesystem drivers will typically not inform the drive that 
blocks are freed; they simply mark in their own data structures that the blocks are not needed, and the data remains on 
the disk until something comes to overwrite it. \cite{Cornwell2012}

SSDs break many of these assumptions. Because the FTL takes the liberty of shuffling data around on the disk as it sees 
fit, I/O coalescing and other location assumptions are meaningless, and the time and effort put in to them are wasted. 
Not informing the drive of freed blocks harms the performance of SSDs, as they rely on having as many free pages as 
possible. \cite{Cornwell2012, Micheloni2013, MatejFucek2014}

\subsection{What this means for databases}

With the direction that computing is headed, data is invaluable. The databases in which this data is stored are completely 
dependent on the storage medium in which they are backed. 

\section{Solutions to these effects}

Text goes here.

\section{Conclusions}

Text goes here.

\bibliographystyle{ACM-Reference-Format}
\bibliography{bibliography}

\end{document}
