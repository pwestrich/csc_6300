
\documentclass[format=acmsmall, review=false, screen=true]{acmart}

\usepackage{booktabs} % For formal tables

\usepackage[ruled]{algorithm2e} % For algorithms
\renewcommand{\algorithmcfname}{ALGORITHM}
\SetAlFnt{\small}
\SetAlCapFnt{\small}
\SetAlCapNameFnt{\small}
\SetAlCapHSkip{0pt}
\IncMargin{-\parindent}

% Copyright
\setcopyright{acmlicensed}

% Document starts
\begin{document}
% Title portion. Note the short title for running heads 
\title[Effects of SSDs on Databases]{Effects of Solid State Drives on Database Performance}  
\author{Philip M. Westrich}
\affiliation{
  \institution{Tennessee Technological University}
  \streetaddress{1 William L. Jones Drive}
  \city{Cookeville} \state{TN} \postcode{38501} \country{USA}
}

\begin{abstract}

    In the last decade or so, a new form of flash-based persistent storage, known as solid state drives (SSDs), have begun 
    to replace traditional mechanical hard disk drives (HDDs). They are orders of magnitude faster, lighter, more energy 
    efficient, and less prone to sudden impact than HDDs. As their prices fall and storage capacities increase, they have 
    become much more popular in both the consumer and enterprise market. These new SSDs have different performance characteristics
    than mechanical drives, which can impact the performance on storage-heavy applications, such as databases. In this paper, 
    we explore what these impacts are and their proposed solutions.
 
\end{abstract}

\maketitle

\section{Introduction}

For years, the gap in performance between processors and persistent storage has continually widened. Disk latency for 
traditional hard disk drives (HDDs) has improved by approximately ten percent per year, but processor speed has nearly 
doubled each year according to Moore's Law until just recently. \cite{Xie2011}

Traditional HDDs rely on mechanical parts, which degrade over time, are prone to multiple methods of failure, such as 
sudden force, vibration, or simply mechanical failure. They also are inefficient at their jobs; they read and write data 
slowly while using lots of energy and generating lots of heat. \cite{Xie2011}

With the introduction of solid state drives (SSDs), they can address many of the aforementioned issues. SSDs are lighter, 
faster, more energy efficient, and resistant to blunt force. \cite{Xie2011} 

However, they are not completely free from issue. Currently, SSDs are still much more expensive per gigabyte than their 
HDD counterparts, though they promise to reach equivalence within a few years. SSDs also have much different performance 
characteristics than HDDs. Most notably, SSDs are usually slower at small random writes. SSDs also tend to become slower 
as they fill up, and have a limited number of read/write/erase cycles they can endure before failing. 
\cite{Dumitru2007, Dirik2009, Xie2011}

Overall, SSDs are better than their predecessor, and will most likely replace HDDs in the coming years. The adoption of 
SSDs will impact applications that are heavily reliant on their storage medium, such as databases. In this paper, we will 
explore what effects these performance differences have as well as the solutions that have been proposed.

\section{Overview of solid state drives}

Solid state drives are designed to emulate the behavior of previous block devices. However, they operate quite differently. 
\cite{Lee2008, Dirik2009, Cornwell2012, Micheloni2013, MatejFucek2014, Chen2016} In this section, we explain the general 
architecture of a solid state drive and explain these differences.

Solid state drives consist of three major components: an array of non-volatile flash memory, a host interface, and a 
microcontroller that bridges the gap between the two. 

\subsection{NAND flash}

Most SSDs today use a type of electrically erasable programmable read-only memory (EEPROM) known as NAND flash memory. 
NAND is an abbreviation for 'not-and', a type of logic gate. These NAND cells are arranged into a grid pattern. These 
cells can only be accessed in certain patterns dependent on the particular drive's arrangement of those cells. 
\cite{Dirik2009, Cornwell2012, Micheloni2013, Chen2016}

Several of these grids, usually one to four, are placed together into what is known as a die. Each die can read somewhere 
around 400 MB/sec, while they can only write at around 20MB/sec. This is due to the much more complicated write method 
they employ. However, they have a much lower latency for these operations, typically measured in microseconds, and in 
order to meet capacity and speed requirements, multiple dies are packaged inside one SSD. The controller will often 
read and write to multiple dies at once in order to reach performance goals. 
\cite{Dirik2009, Cornwell2012, Micheloni2013, Chen2016}

SSDs use a write-verify method to program, or write to, the cells. In this procedure, the cells are first erased, then 
a high voltage signal is applied to them until they reach the appropriate value. As the cells hold more bits, this 
process gets more difficult and slows down, as the voltages applied to the cells must become more fine-grained and precise.
\cite{Dirik2009, Cornwell2012, Micheloni2013, Chen2016}

\subsection{SSD controller}

The controller has many jobs. First, it must present to the host device the same interface HDDs do. Second, it must 
translate the commands issued to it to operations on the NAND flash. Third, it must wear the NAND in the drive evenly 
to prevent premature failure, which is known as wear leveling. Fourth, it must deal with the higher error rate that 
the NAND flash has over HDDs. \cite{Dirik2009, Cornwell2012, Micheloni2013, MatejFucek2014, Chen2016}

To carry out these tasks, SSDs have a small computer, known as a microcontroller, embedded within them. The microcontroller 
is also paired with specialized hardware to speed up some of its computations, as well as some DRAM to cache and buffer 
events. \cite{Dirik2009, Cornwell2012, Micheloni2013, MatejFucek2014, Chen2016}

The controller must present to the host device the same interface other block devices do. However, the NAND flash is not 
accessed in the same way; instead, it has its own filesystem it maintains, known as the flash translation layer (FTL). 
The FTL is used to perform the translation from the logical blocks the host requests to the pages and blocks within the 
NAND flash. \cite{Cornwell2012, Micheloni2013, MatejFucek2014, Chen2016}

Because cells must be erased before they can be written to, new data is always written to known empty pages, and the old 
pages are marked as garbage in the FTL. The actual locations of data may move across the drive as it is written to, unlike 
HDDs. The FTL must maintain a list of empty, written, and garbage pages, and keep track of the mapping from the logical 
blocks requested by the host and their actual location on the SSD. The FTL also maintains the number of writes a block 
has experienced as well as a list of bad blocks that have either started out defective from the factory, or have worn out 
over time. \cite{Dirik2009, Cornwell2012, Micheloni2013, MatejFucek2014, Chen2016}

When the controller is idle for some period of time, it will start performing garbage collection on the marked invalid 
pages in order to free up more space. Some SSDs, especially earlier ones, would degrade in performance as the drive filled 
up and it ran out of free pages. The drive then had to immediately garbage collect to free up more, causing a delay in writes. 
Modern SSDs will understate their capacity to the host system, that way there are always free pages on the disk. 
\cite{Dirik2009, Cornwell2012, Micheloni2013, MatejFucek2014, Chen2016}

NAND flash has a much higher error rate than the magnetic platters of HDDs. All SSDs have some amount of hardware-
assisted error correction to make up for this. Higher quality drives have more dedicated hardware, while cheaper ones 
may share hardware for a larger number of pages. \cite{Dirik2009, Cornwell2012, Micheloni2013, MatejFucek2014, Chen2016}

\section{Effects on database systems}

For years, systems have assumed that HDDs are the form of persistent storage used, and are optimized for it. These 
optimizations often are detrimental to SSD performance. In this section, we explain these assumptions and why they 
may lead to suboptimal performance.

\subsection{Physical differences}

HDDs have multiple moving parts: the platters that spin at some speed, and the heads that move across them that read the 
data from the platters. It takes time to move these parts through space. Often, to conserve power and lifespan, the drive 
stops rotating its platters. When at a complete stop, it can take between four to ten seconds to bring the platters back 
up to speed. This may not be an issue in consumer-grade applications, but it can be a matter of life and death in critical
real-time systems. \cite{MatejFucek2014} The head of the drive must also move to the correct location to read from the 
requested sector. \cite{Cornwell2012}

Solid state drives have zero moving parts; that is where the name solid-state comes from. They do not require any spin up 
time, and their seek time is both uniform and much, much smaller. An additional benefit is that they consume much less 
energy. \cite{Cornwell2012, Micheloni2013, MatejFucek2014}

\subsection{Differences in access patterns}

Current systems tend to assume that they will be placing their data on to HDDs, and tend to optimize for that; they try 
to make up for the shortcomings of the physical media. They assume the location of the sectors requested are always in 
the same location, and that lower addresses are towards the faster, outside of the drive, and higher addresses are towards 
the slower inside. \cite{Cornwell2012}

Because of the seek time added by moving the head across the drive and the assumption of block locations, filesystem drivers 
will compute complicated heuristics to determine how it can move the head the minimum distance for a sequence of reads 
and writes. They will also perform \textit{I/O coalescing,} or merging many smaller requests into one larger one, since 
HDDs are much better at large sequential operations. \cite{Cornwell2012}

In order to minimize the number of requests made to the drive, filesystem drivers will typically not inform the drive that 
blocks are freed; they simply mark in their own data structures that the blocks are not needed, and the data remains on 
the disk until something comes to overwrite it. \cite{Cornwell2012}

SSDs break many of these assumptions. Because the FTL takes the liberty of shuffling data around on the disk as it sees 
fit, I/O coalescing and other location assumptions are meaningless, and the time and effort put in to them are wasted. 
Not informing the drive of freed blocks harms the performance of SSDs, as they rely on having as many free pages as 
possible. \cite{Cornwell2012, Micheloni2013, MatejFucek2014}

\subsection{What this means for databases}

With the direction that computing is headed, data is invaluable. Data-heavy applications, such as those that heavily use 
online transaction processing (OLTP), require a data store that provides high performance and reliability.  The databases 
in which this data is stored are completely dependent on the storage medium that backs them. Its characteristics can 
greatly impact the end performance of the system. \cite{Lee2009, Xie2011, Micheloni2013, MatejFucek2014, Chen2016}

OLTP systems often require many small, random writes, which are the one thing that SSDs tend to perform worse at over 
HDDs. A small write requires rewriting the entire page with its changes to a separate location in the drive, then erasing 
the previous one, which is a costly operation. Many of these mall writes performed close together may lead to the SSD 
running out of free pages to write to, which would degrade performance tremendously. 
\cite{Cornwell2012, Micheloni2013, MatejFucek2014, Chen2016}

Other systems that have a more read-heavy workload will improve greatly with the introduction of SSDs, especially as the 
randomness of those reads increases. Their only worry would then be the higher cost of large storage, which promises to 
eventually fall to the price of current HDDs. \cite{Chen2016}

\section{Solutions to these effects}

Some have already explored replacing HDDs with SSDs in the enterprise space, and have proposed some solutions to the 
problems that they introduce. In this section, we explain these solutions.

\subsection{Hardware improvements}

SSD manufacturers have made numerous advances on their end to help mitigate what SSDs are bad at. There are several 
places apt for improvement, such as the FTL, or simply the amount of hardware included in the device. The more efficient 
the FTL is at translating and ordering requests and the more hardware that allows for increased parallelism, the more 
performance will come from the device. \cite{Xie2011, Chen2016}

Many SSDs share hardware for both error correction and the write-verify cycle. Enterprise-grade devices tend to have 
less hardware sharing, increasing the amount of parallelism; areas of the drive will not have to wait in line to use 
the shared hardware. \cite{Dirik2009, Cornwell2012, Micheloni2013, MatejFucek2014, Chen2016}

An improved interface for the device will also increase SSD performance. Currently, SSDs use the same interface that 
HDDs have employed, the Serial Advanced Technology Attachment generation 3 (SATA-3), which is limited to somewhere around
600 MB/sec of thourughput. While that may be sufficent for HDDs, SSDs can easily outpace that. \cite{Micheloni2013}

Fortunately, there are faster interfaces present on modern day systems. The Peripheral Component Interconnect Express 
(PCIe) interface is used in all modern-day systems to connect important devices together. In fact, the SATA controller 
is often itself a PCIe device. PCIe promises transfer speeds anywhere from two to eight times that of SATA dependent on 
the number of lanes dedicated to the device. It also supports duplex and device-to-device communication, eliminating 
overhead. \cite{Micheloni2013}

Moving SSDs to PCIe would allow for much higher throughput, lower latency, and simplification of the FTL; it would no 
longer need to emulate a traditional block device. Other PCIe devices could also directly interact with the SSD to move 
data, reducing communication overhead with the CPU. \cite{Micheloni2013}

\subsection{Software improvements}

There are also a number of software improvements that can be made to take advantage of what flash-based storage is good 
at and helping to mitigate what they are bad at, much like what current filesystem drivers have done with magnetic media.

Operating systems file drivers will need to become aware of flash-based media. We have seen this recently, for example, 
with Apple's introduction of the Apple File System (APFS). It promises to bring with it a number improvements, most notably
in the case of this papaer, being optimized to work with flash-based storage. \cite{AppleComputer2017}

Systems that are data-heavy will need to attempt to optimize their access patterns for SSDs rather than HDDs. Knowing that 
SSDs are slow at small, random writes, systems will want to avoid that behavior. Previous assumptions based on the workings 
of HDDs can be disregarded. 

\section{Conclusions}

Overall, solid state drives are a huge performance improvement over rotational magnetic media. They help greatly in closing 
the performance gap between processors and storage that we have seen widen in the last several decades. It will only be a 
matter of time before SSDs reach the same data density and cost per gigabyte of HDDs. \cite{Chen2016}

However, SSDs introduce new problems in applications due to the vastly different behaviors that they exhibit. Both software 
and hardware will need to start accommodating for these differences in order to reduce bottlenecks in data-intensive 
applications, such as database management systems. We have already seen this with the works in \cite{AppleComputer2017}, 
and more is sure to come.

\bibliographystyle{ACM-Reference-Format}
\bibliography{bibliography}

\end{document}
